{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiddhcx/ECE720/blob/main/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5a8ad2e"
      },
      "source": [
        "# ECE 720 Assignment Hands-on Project 2"
      ],
      "id": "b5a8ad2e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90d5331d"
      },
      "source": [
        "In this assignment, we will explore how to test a DL model.\n",
        "\n",
        "Before working on the assignment, it is highly recommended to read two papers: [DeepGauge: Multi-Granularity Testing Criteria for Deep Learning Systems](https://arxiv.org/pdf/1803.07519.pdf) and [DeepHunter: A Coverage-Guided Fuzz Testing Framework for Deep Neural Networks](https://dl.acm.org/doi/pdf/10.1145/3293882.3330579?casa_token=mGUOGHi9qrcAAAAA:G28BWlY6XueMERjYHKWyW7w5jfGQrzF3fyml6ThPX0Rq8yUE4NYTz28s83fllXvaRItcpflNbBtdEYs). Our materials will be highly relevant to these two papers.\n"
      ],
      "id": "90d5331d"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyXTIyYSZ7Mh",
        "outputId": "60ad4379-9b53-4f89-b801-338a423bc19d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-05 03:59:40--  https://github.com/momentum-openspace/momentum-openspace.github.io/raw/new-template/files/asg2/asg2.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/momentum-openspace/momentum-openspace.github.io/new-template/files/asg2/asg2.zip [following]\n",
            "--2022-03-05 03:59:40--  https://raw.githubusercontent.com/momentum-openspace/momentum-openspace.github.io/new-template/files/asg2/asg2.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 770861 (753K) [application/zip]\n",
            "Saving to: ‘asg2.zip.1’\n",
            "\n",
            "asg2.zip.1          100%[===================>] 752.79K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2022-03-05 03:59:41 (12.6 MB/s) - ‘asg2.zip.1’ saved [770861/770861]\n",
            "\n",
            "Archive:  asg2.zip\n",
            "replace all_utils.cpython-37.pyc? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: all_utils.cpython-37.pyc  \n",
            "  inflating: __MACOSX/._all_utils.cpython-37.pyc  \n",
            "  inflating: all_utils.cpython-38.pyc  \n",
            "  inflating: __MACOSX/._all_utils.cpython-38.pyc  \n",
            "  inflating: all_utils.cpython-39.pyc  \n",
            "  inflating: __MACOSX/._all_utils.cpython-39.pyc  \n",
            "  inflating: lenet.pt                \n",
            "  inflating: pic/Intro.png           \n",
            "  inflating: pic/traditional.png     \n",
            "  inflating: pic/alg_2.png           \n",
            "  inflating: pic/alg_1.png           \n",
            "  inflating: pic/lenet_vis.png       \n",
            "  inflating: pic/AI.png              \n",
            "  inflating: train_idx.pt            \n"
          ]
        }
      ],
      "source": [
        "! wget https://github.com/momentum-openspace/momentum-openspace.github.io/raw/new-template/files/asg2/asg2.zip\n",
        "! unzip asg2.zip"
      ],
      "id": "uyXTIyYSZ7Mh"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DtTjWTr62YZS",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import imp\n",
        "import platform\n",
        "\n",
        "py_version = platform.python_version().split('.')[1]\n",
        "if py_version == '7':\n",
        "    all_utils = imp.load_compiled(\"all_utils\", \"./all_utils.cpython-37.pyc\")\n",
        "elif py_version == '8':\n",
        "    all_utils = imp.load_compiled(\"all_utils\", \"./all_utils.cpython-38.pyc\")\n",
        "elif py_version == '9':\n",
        "    all_utils = imp.load_compiled(\"all_utils\", \"./all_utils.cpython-39.pyc\")\n",
        "else:\n",
        "    raise NotImplementedError('Only supports Python>=3.7')"
      ],
      "id": "DtTjWTr62YZS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01af4c0a"
      },
      "source": [
        "### Content:\n",
        "\n",
        "1. The difference between traditional software and AI model <font color='red'> (4 points)</font>\n",
        "\n",
        "    - Please summarize the characteristics of traditional software and compare it to AI model (2 points)\n",
        "    - Please summarize the characteristics of AI model and compare it to traditional software (2 points)\n",
        "    \n",
        "    \n",
        "2. Quality assurance of DL models <font color='red'> (13 points)</font>\n",
        "\n",
        "    2.1 Testing metric <font color='red'> (7 points)</font>\n",
        "    \n",
        "    - Profile the DNN on training data (3 points)\n",
        "    - Implement Neuron Boundary Coverage (4 points)\n",
        "    \n",
        "    2.2 Mutation <font color='red'> (6 points)</font>\n",
        "\n",
        "    - Answer the question (2 points)\n",
        "    - Complete Algorithm2 (4 points)\n",
        "\n",
        "3. Experiments <font color='red'> (3 points)</font>\n",
        "    \n",
        "    - Conduct experiments on mutation testing.\n"
      ],
      "id": "01af4c0a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7863c4e"
      },
      "source": [
        "## 1. The difference between traditional software and AI model <font color='red' size=5> (4 points)</font>"
      ],
      "id": "d7863c4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9140225"
      },
      "source": [
        "![Intro.png](https://raw.githubusercontent.com/momentum-openspace/momentum-openspace.github.io/new-template/files/asg2/pic/Intro.png)"
      ],
      "id": "e9140225"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a61d722"
      },
      "source": [
        "<center> Difference between traditional software and AI models (From DeepHunter)</center>"
      ],
      "id": "5a61d722"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fc0d5db5"
      },
      "outputs": [],
      "source": [
        "# Load necessary package for the experiments\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_image(image):\n",
        "    fig = plt.figure()\n",
        "    image = np.squeeze(image)\n",
        "    plt.imshow(image, cmap='bone')"
      ],
      "id": "fc0d5db5"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "52807591",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Get ready for MNIST dataset\n",
        "data_path = './data'\n",
        "train_data = datasets.MNIST(root=data_path, train=True, download=True)"
      ],
      "id": "52807591"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cefb5c19"
      },
      "source": [
        "### 1.1 Traditional software"
      ],
      "id": "cefb5c19"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "932cbd9a"
      },
      "source": [
        "Now assume you are a developer who wants to develop a software for recognizing hand-written digit."
      ],
      "id": "932cbd9a"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "32efa840"
      },
      "outputs": [],
      "source": [
        "# Turn images into numpy array.\n",
        "number_a = np.array(train_data[0][0])\n",
        "number_b = np.array(train_data[600][0])"
      ],
      "id": "32efa840"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "1331717d",
        "outputId": "01cbd0ae-2d12-4caa-a564-7a032d341e5e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Image b')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Visualize two examples.\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1, 2, 1)\n",
        "ax.imshow(number_a, cmap='bone')\n",
        "ax.set_xlabel(\"Image a\")\n",
        "ax = fig.add_subplot(1, 2, 2)\n",
        "ax.imshow(number_b, cmap='bone')\n",
        "ax.set_xlabel(\"Image b\")"
      ],
      "id": "1331717d"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e22fc43",
        "outputId": "e2553185-8c36-4361-d2d4-efe1d1d5e5d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sum of pixel value of image A is: 27525\n",
            "The sum of pixel value of image B is: 15734\n"
          ]
        }
      ],
      "source": [
        "# Compute the sum of pixel value of each images.\n",
        "print(\"The sum of pixel value of image A is: {}\".format(np.sum(number_a.reshape(-1))))\n",
        "print(\"The sum of pixel value of image B is: {}\".format(np.sum(number_b.reshape(-1))))"
      ],
      "id": "3e22fc43"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "188fc137"
      },
      "source": [
        "You may go through the examples above and find that number \"5\" has larger sum of pixel value than that of number \"9\", based on this observation, a naive judegement could be made: For those images with sum of pixel value is above `20000`, it is the digit `5`, otherwise it is the digit `9`"
      ],
      "id": "188fc137"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "164de35e",
        "outputId": "31971ec6-51f1-419e-d3f6-36944b53575c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image a is 5\n",
            "Image b is 9\n"
          ]
        }
      ],
      "source": [
        "def recognize_digit(image):\n",
        "    # --- Below is the logic of this function ---\n",
        "    if np.sum(image.reshape(-1)) > 20000:\n",
        "        return 5\n",
        "    else:\n",
        "        return 9\n",
        "    # --- Above is the logic of this function ---\n",
        "\n",
        "print(\"Image a is {}\".format(recognize_digit(number_a)) )\n",
        "print(\"Image b is {}\".format(recognize_digit(number_b)) )"
      ],
      "id": "164de35e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbc7e130"
      },
      "source": [
        "<font color='blue' size=4> Please summarize the characteristics of traditional software and compare it to AI model</font> <font color='red' size=4> (2 points)</font>"
      ],
      "id": "fbc7e130"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-Hg-KvCtWc7"
      },
      "source": [
        "![traditional.png](https://raw.githubusercontent.com/momentum-openspace/momentum-openspace.github.io/new-template/files/asg2/pic/traditional.png)"
      ],
      "id": "T-Hg-KvCtWc7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb9a17ec"
      },
      "source": [
        "### 1.2 AI model"
      ],
      "id": "eb9a17ec"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d91bd88f"
      },
      "source": [
        "Now let's see the AI model. We have a pre-trained AI model ready for you and let's load it."
      ],
      "id": "d91bd88f"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "b87b3901"
      },
      "outputs": [],
      "source": [
        "from all_utils import Lenet5\n",
        "from all_utils import load_model\n",
        "\n",
        "model = load_model(Lenet5, \"./lenet.pt\")"
      ],
      "id": "b87b3901"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94bbf6d3"
      },
      "source": [
        "<center> The AI model architecture </center>"
      ],
      "id": "94bbf6d3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee6fe059"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/momentum-openspace/momentum-openspace.github.io/new-template/files/asg2/pic/lenet_vis.png\" height=1000 weight=600> "
      ],
      "id": "ee6fe059"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ddd9b44"
      },
      "source": [
        "The above figure shows the computation graph of the AI model. In each element, there are some computations going on. All the blue components with \".weight\" can also be taken as \"neurons.\" \n",
        "\n",
        "Let's predict the image using AI model."
      ],
      "id": "3ddd9b44"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee1713d1",
        "outputId": "05b384c6-4f00-4715-b2f0-60f468e4f106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image a is 5\n",
            "Image b is 9\n"
          ]
        }
      ],
      "source": [
        "def recognize_digit_AI(image, model):\n",
        "    tensor = all_utils.image_to_tensor(image)\n",
        "    # --- Below is the prediction made by the model ---\n",
        "    with torch.no_grad():\n",
        "        result = model(tensor)\n",
        "    # --- Above is the prediction made by the model ---\n",
        "    return np.argmax(result.cpu(), axis = 1)[0]\n",
        "\n",
        "print(\"Image a is {}\".format(recognize_digit_AI(number_a, model)) )\n",
        "print(\"Image b is {}\".format(recognize_digit_AI(number_b, model)) )"
      ],
      "id": "ee1713d1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c88ff572"
      },
      "source": [
        "<font color='blue' size=4> Please summarize the characteristics of AI model and compare it to traditional software </font> <font color='red' size=4> (2 points)</font>"
      ],
      "id": "c88ff572"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7563f45a"
      },
      "source": [
        "Hint: Neurons in the AI model can interact with each other. They are connected with complex non-linear activation functions."
      ],
      "id": "7563f45a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5306096"
      },
      "source": [
        "![Title](https://raw.githubusercontent.com/momentum-openspace/momentum-openspace.github.io/new-template/files/asg2/pic/AI.png)"
      ],
      "id": "b5306096"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f13e7478"
      },
      "source": [
        "## 2. Quality assurance of DL models <font color='red' size=5> (13 points)</font>"
      ],
      "id": "f13e7478"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64b7cb08"
      },
      "source": [
        "So far, we have explored the differences between deep learning models and traditional software. Now let's think about one of the main topics in this course: how to improve the quality of machine learning systems. \n",
        "\n",
        "To do this, let's first review the developing process of classical software:\n",
        "\n",
        "`Software development --> Software testing --> Software debugging --> Software development.`\n",
        "\n",
        "And machine learning systems have similar process:\n",
        "\n",
        "`Model training --> Model evaluation --> Model debugging/ Architecture improvement --> Model (re)training.`\n",
        "\n",
        "Normally Model evaluation is done by testing on test set. Similar to the software testing procedure, this is a very important step towards quality assurance of DL models. However, there are multiple questions remaining:\n",
        "\n",
        "1. The size of the test set is usually limited, because it could be expensive to collect data with labeled data. Hence it's hard to cover all the corner cases in the test set. \n",
        "\n",
        "    a. To solve this problem, we need to develop a method to generate more test cases. \n",
        "    b. Like test generation in traditional automated software testing, we need to generate \"valid\" test cases. For example, you can not generate a cat image to test the handwritten digit recognition system.\n",
        "    \n",
        "\n",
        "2. Let's suppose that you now have a large enough test set. The next question is, how can we tell whether the ML system has been comprehensively tested or not. Or, in other words, what is the criteria to stop the testing and how to measure whether the ML system has been well-tested?\n",
        "\n",
        "    a. The \"code coverage\" metric is usually used in traditional software, and we also have practiced it in the last assignment. If we know that every logical branch in the software has been covered in the testing, we can be more confident to say that this software has been comprehensive tested.  \n",
        "    \n",
        "    \n",
        "3. Now if you have a testing metric like \"code coverage\" and a test case generation method for testing machine learning systems, the next step is to generate as diverse test cases as you can to improve the testing coverage. Is it possible to \"guide\" the test case generation process instead of do it randomly?\n",
        "\n",
        "We will cover all these three questions in the following"
      ],
      "id": "64b7cb08"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06b69b7f"
      },
      "source": [
        "### Goal: Test AI models efficiently\n",
        "\n",
        "### TODO:\n",
        "\n",
        "### 1. Define a testing metric that is suitable for testing DL models.\n",
        "    \n",
        "### 2. Find possible test cases generation methods.\n",
        "    \n",
        "### 3. Design metric-guidance testing technique."
      ],
      "id": "06b69b7f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "601bb649"
      },
      "source": [
        "### 2.1 Testing metric <font color='red' size=3> (7 points)</font>"
      ],
      "id": "601bb649"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0b2e15c"
      },
      "source": [
        "#### 2.1.1 Definition of \"neuron\""
      ],
      "id": "d0b2e15c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f6536e3"
      },
      "source": [
        "In a DNN with only linear layer enabled ([MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron)), the term \"neuron\" represents a computing unit. Each neuron is fully-connected with all neurons on the next layer, and each connection has a weight. \n",
        "\n",
        "`output = linear_layer(input)`.\n",
        "    \n",
        "However, in another kind of neural network called Convolutional Neural Networks (CNNs), which is known more prevalent in the image classification task, there is a special computation unit called \"convolution\". We refer the reader to a nice resource from Stanford University: [Convolutional Neural Networks (CNNs / ConvNets)](https://cs231n.github.io/convolutional-networks/) if you are not familiar with CNNs. Generally speaking, it is more ambiguous to describe a \"neuron\" in CNNs. We will follow the insights in the aforementioned material:\n",
        "\n",
        "> Every entry in the 3D output volume can also be interpreted as an output of a neuron that looks at only a small region in the input and shares parameters with all neurons to the left and right spatially (since these numbers all result from applying the same filter).\n",
        "    \n"
      ],
      "id": "0f6536e3"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2919d5b2",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Before the experiment, let's first load the data\n",
        "train_loader = all_utils.get_training_loader(\"./train_idx.pt\")\n",
        "test_loader = all_utils.get_test_loader()"
      ],
      "id": "2919d5b2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d47c78ef"
      },
      "source": [
        "Notice: \n",
        "\n",
        "Here the train_loader and test_loader are `torch.utils.data.DataLoader` (reference: [dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)). It works like an iterator that automatically returns you the image in each `batch`. The batch size is set to 64 here. In other words, for the tensor returned by the loader, the returned tensor shape is (64, 1, 28, 28), where 64 is the batch_size, 1 is the number of channel, the remaining two dimensions are the height and width of the image. \n",
        "\n",
        "(64, 1, 28, 28) --> (N, C, H, W)\n",
        "\n",
        "While doing image processing, the image is usually in the format (H, W, C), where channel is in the last dimension.\n",
        "\n",
        "You may wonder why pytorch chooses (N, C, H, W). This is because in this format the memory access pattern is beneficial for convolution implementations. If you are interested for this, you can check this [presentation](https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9926-tensor-core-performance-the-ultimate-guide.pdf)"
      ],
      "id": "d47c78ef"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33d7f961"
      },
      "source": [
        "#### 2.1.2 Introduction to DNN testing metric: Neuron Boundary Coverage"
      ],
      "id": "33d7f961"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61481c82"
      },
      "source": [
        "In this section we will explore the testing metric at Neuron-level. You can check for the paper [DeepGauge](https://dl.acm.org/doi/pdf/10.1145/3238147.3238202?casa_token=xNahdy3nsNUAAAAA:FVq-aGLpMPYO9s2MgstFsej05GTcGLCnaOZzse1k61C92jqyli4iCYLFHYopNk8PIPzqbVSwDqssC4I) for more information.\n",
        "\n",
        "Intuitively, we have \"code coverage\" for classical software. If we takes the neurons as the basic components in a DNN, we can also define neuron-based coverage metric to measure how good the testing is and guide the testing. This is the basic logic behind Neuron-Level Coverage Criteria in [DeepGauge](https://dl.acm.org/doi/pdf/10.1145/3238147.3238202?casa_token=xNahdy3nsNUAAAAA:FVq-aGLpMPYO9s2MgstFsej05GTcGLCnaOZzse1k61C92jqyli4iCYLFHYopNk8PIPzqbVSwDqssC4I). \n",
        "\n",
        "The functionality (i.e., neuron output) for each neuron of a DNN should follow some statistical distribution, and this distribution is implicitly determined by the training data. The output distribution analysis would allow to approximately characterize how wide the range of each neuron's output on the training data."
      ],
      "id": "61481c82"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfb5706b"
      },
      "source": [
        "So, we can first analyze the output distribution of a neuron in the training data, find its distribution, and then analyze the output distribution of a neuron in the test data. If the later one \"covers\" the former, we can conclude that the testing data has covered some cases which the model never saw in the training process.\n",
        "\n",
        "Now let's take a look at the Neuron Boundary Coverage (NBC)."
      ],
      "id": "cfb5706b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f441a00f"
      },
      "source": [
        "<font color='red' size=3> Definition 1</font> \n",
        "\n",
        "For a neuron n, let $high_n$ and $low_n$ be its upper and lower boundary output values, respectively, on the value range of its activation function, where $high_n$ and $low_n$ are derived from the training dataset analysis.\n",
        "\n",
        "<font color='red' size=3> Definition 2</font> \n",
        "\n",
        "For a test input $x \\in T$ , we say that a DNN is located in its major function region given x iff $\\forall n \\in N : \\phi(x,n) \\in [low_n, high_n]$ ."
      ],
      "id": "f441a00f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "570463d4"
      },
      "source": [
        "As you can see, here we define the \"distribution\" of a neuron by measuring its **output range**. Based on this, we then define:"
      ],
      "id": "570463d4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0fefe8f"
      },
      "source": [
        "<font color='red' size=3> Definition 3</font> \n",
        "\n",
        "UpperCornerNeuron = {$n \\in N | \\exists x \\in T: \\phi(x, n) \\in (high_n, +\\infty)$};\n",
        "\n",
        "LowerCornerNeuron = {$n \\in N | \\exists x \\in T: \\phi(x, n) \\in (-\\infty, low_n)$};"
      ],
      "id": "b0fefe8f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfca3352"
      },
      "source": [
        "Definition 3 define two types of situations where the output of neuron is in the corner-case regions (Not present in the training data.) We assume that if at test time, the more corner cases you find, the better chance you can test the DNN thoroughly."
      ],
      "id": "bfca3352"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "471755aa"
      },
      "source": [
        "Finally, we give the definition of Neuron Boundary Coverage:\n",
        "\n",
        "<font color='red' size=3> Definition 4</font>\n",
        "\n",
        "NBCov(T) = $\\frac{|UpperCornerNeuron| + |LowerCornerNeuron|}{2 \\times |N|}$\n",
        "\n",
        "Where $|\\cdot|$ indicates the number, e.g., $|N|$ indicates the number of all neurons."
      ],
      "id": "471755aa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6f8a9a9"
      },
      "source": [
        "**In the following, we will implement NBC.**"
      ],
      "id": "b6f8a9a9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06cf873e"
      },
      "source": [
        "<font color='blue' size=4> Profile the DNN on training data</font>"
      ],
      "id": "06cf873e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d17af30d"
      },
      "source": [
        "We will first profile the DNN on training data to get the output range.\n",
        "\n",
        "(What we are doing here is just like instrumentation in traditional software testing.)\n",
        "\n",
        "To get the output of each neuron, we need to get the output of each intermediate layer. For PyTorch, this can be done using hook. If you are interested in thie technique, you can check this video: [PyTorch Hooks Explained - In-depth Tutorial](https://www.youtube.com/watch?v=syLFCVYua6Q). While in this assignment we have implemented a function called `feature_list` that returns all the intermediate result for you:\n",
        "\n",
        "```\n",
        "logits, out_list, name_list = model.feature_list(data)\n",
        "# logits: the final output\n",
        "# output_list: the output of each intermediate layer\n",
        "# name_list: the list of names for each intermediate layer\n",
        "```\n",
        "\n",
        "Use this function, let's profile the DNN"
      ],
      "id": "d17af30d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0fc0081"
      },
      "source": [
        "<font color='blue' size=3> Please complete the todo listed in the function shown below</font> <font color='red' size=3> (3 points)</font>"
      ],
      "id": "e0fc0081"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16b328ce"
      },
      "source": [
        "Functions that you \\*may\\* need to use here:\n",
        "\n",
        "[function Tensor.reshape](https://pytorch.org/docs/stable/generated/torch.reshape.html)\n",
        "\n",
        "    Returns a tensor with the same data and number of elements as input, but with the specified shape. \n",
        "    `torch.tensor([1, 2]).reshape`\n",
        "\n",
        "[function Tensor.max](https://pytorch.org/docs/stable/generated/torch.max.html?highlight=max#torch.max)\n",
        "    \n",
        "    Returns the maximum value of all elements in the input tensor.\n",
        "    `torch.tensor([1, 2]).max`\n",
        "\n",
        "[function Tensor.min](https://pytorch.org/docs/stable/generated/torch.min.html?highlight=min#torch.min)\n",
        "    \n",
        "    Returns the minimum value of all elements in the input tensor.\n",
        "    `torch.tensor([1, 2]).min`\n",
        "\n",
        "[torch.flatten](https://pytorch.org/docs/stable/generated/torch.flatten.html?highlight=flatten#torch.flatten)\n",
        "    \n",
        "    Flattens input by reshaping it into a one-dimensional tensor. "
      ],
      "id": "16b328ce"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "23dff3db"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "def profile_DNN(net, dataloader):\n",
        "    \"\"\"\n",
        "    Profile the DNN and return a cov_dict contains the necessary information\n",
        "    ---\n",
        "    Args\n",
        "        net: torch.nn.Module Deep learning model.\n",
        "        dataloader: torch.utils.data.DataLoader The pytorch dataloader.\n",
        "    Returns:\n",
        "        cov_dict: a dict contains (layer_name, neuron_id) --> (neurons_min, neurons_max) mapping.\n",
        "    \n",
        "    \"\"\"\n",
        "    net.eval()\n",
        "    cov_dict = collections.OrderedDict()\n",
        "    batch_num = len(dataloader)\n",
        "    for idx, (data, label) in enumerate(dataloader):\n",
        "        with torch.no_grad():\n",
        "            logits, out_list, name_list = net.feature_list(data)\n",
        "        if idx % 100 == 0:\n",
        "            print(\"* finish {}/{} proportion\".format(idx, batch_num))\n",
        "        \n",
        "        # Iterate through all output layers in the output list\n",
        "        for layer_id in range(len(out_list)):\n",
        "            cur_layer = name_list[layer_id] # record the name of current layer\n",
        "            \n",
        "            '''\n",
        "            Recall the definition of neurons. For a linear layer, the output is [Batch_size, neurons]\n",
        "            While for a convolutional layer, the output is [Batch_size, channel, H, w]\n",
        "            As we define that every entry in the 3D output volume is one neuron, \n",
        "            the number of neurons in conv layer is just channel * H * W.\n",
        "            '''\n",
        "            \n",
        "            cur_neuron_num = 1\n",
        "            \n",
        "            '''\n",
        "            Please compute the number of neurons in the DNN and store the result in cur_neuron_num.\n",
        "            '''\n",
        "            \n",
        "            # ======Below is TODO====== #\n",
        "            if len(out_list[layer_id].shape) == 4:\n",
        "              cur_neuron_num = out_list[layer_id].shape[1] * out_list[layer_id].shape[2] * out_list[layer_id].shape[3]\n",
        "            else:\n",
        "              cur_neuron_num = out_list[layer_id].shape[1]\n",
        "            # ======Above is TODO====== #\n",
        "            \n",
        "            '''\n",
        "            Please records the max value and min value over **batch** (the first dimension) from the **out_list**\n",
        "            Hint: you can use torch.flatten or reshape to turn a multi-dimension tensor\n",
        "            into one-dimension tensor to [C, H, W] --> [N] can be easily used to represent neurons.\n",
        "            Data structure used here: out_list, neurons_max, neurons_min\n",
        "            '''\n",
        "            \n",
        "            # ======Below is TODO====== #\n",
        "            out_list[layer_id] = torch.flatten(out_list[layer_id],start_dim=1)\n",
        "            neurons_max = torch.max(out_list[layer_id],0)\n",
        "            neurons_min = torch.min(out_list[layer_id],0)\n",
        "            \n",
        "            # ======Above is TODO====== #\n",
        "            for neuron_id in range(cur_neuron_num):\n",
        "                # Please record the range of the neuron for key (cur_layer, neuron_id).\n",
        "                if (cur_layer, neuron_id) not in cov_dict:\n",
        "                    # The fisrt time to record the pair.\n",
        "                    cov_dict[(cur_layer, neuron_id)] = [None, None] # Initialize [Lower, upper]\n",
        "                \n",
        "                profile_data_list = cov_dict[(cur_layer, neuron_id)]\n",
        "\n",
        "                '''\n",
        "                Please compare the result in neurons_max, neurons_min with that in profile_data_list\n",
        "                If neurons_max is larger than the data in profile_data_list\n",
        "                store it in the upper_bound and store back to the profile_data_list.\n",
        "                Data structure used here: lower_bound, upper_bound, neurons_min, neurons_max\n",
        "                '''\n",
        "                \n",
        "                # ======Below is TODO====== #\n",
        "                if profile_data_list == [None,None]:\n",
        "                  upper_bound = neurons_max.values[neuron_id]\n",
        "                  lower_bound = neurons_min.values[neuron_id]\n",
        "                else:\n",
        "                  if neurons_max.values[neuron_id] > profile_data_list[1]:\n",
        "                    upper_bound = neurons_max.values[neuron_id]\n",
        "                  else:\n",
        "                    upper_bound = profile_data_list[1]\n",
        "                  if neurons_min.values[neuron_id] < profile_data_list[0]:\n",
        "                    lower_bound = neurons_min.values[neuron_id]\n",
        "                  else:\n",
        "                    lower_bound = profile_data_list[0]\n",
        "                # ======Above is TODO====== #\n",
        "\n",
        "                profile_data_list[0] = lower_bound\n",
        "                profile_data_list[1] = upper_bound\n",
        "                cov_dict[(cur_layer, neuron_id)] = profile_data_list  \n",
        "    return cov_dict"
      ],
      "id": "23dff3db"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "836f8188"
      },
      "outputs": [],
      "source": [
        "from all_utils import check_cov_list"
      ],
      "id": "836f8188"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "122feebc",
        "outputId": "5ed35cc9-0c83-4826-c5aa-54bee37e0c79",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* finish 0/750 proportion\n",
            "* finish 100/750 proportion\n",
            "* finish 200/750 proportion\n",
            "* finish 300/750 proportion\n",
            "* finish 400/750 proportion\n",
            "* finish 500/750 proportion\n",
            "* finish 600/750 proportion\n",
            "* finish 700/750 proportion\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Congratulation! You have passed the sanity check'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "train_cov_dict = profile_DNN(model, train_loader)\n",
        "check_cov_list(train_cov_dict)"
      ],
      "id": "122feebc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04fc431d"
      },
      "source": [
        "Well Done! Let's now implement the NBC metric"
      ],
      "id": "04fc431d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e7d4e55"
      },
      "source": [
        "<font color='blue' size=4> Implement NBC</font>"
      ],
      "id": "6e7d4e55"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b90a5c2d"
      },
      "source": [
        "<font color='blue' size=3> Please complete the todo listed in the function shown below</font> <font color='red' size=3> (4 points)</font>"
      ],
      "id": "b90a5c2d"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1f17b3f7"
      },
      "outputs": [],
      "source": [
        "def compute_NBCov(net, dataloader, cov_dict):\n",
        "    \"\"\"\n",
        "    Compute the Neuron Boundary Coverage metric on a given data loader.\n",
        "    ---\n",
        "    Args\n",
        "        net: torch.nn.Module Deep learning model.\n",
        "        dataloader: torch.utils.data.DataLoader The pytorch dataloader.\n",
        "        cov_dict: a dict contains (layer_name, neuron_id) --> (neurons_min, neurons_max) mapping.\n",
        "    Returns:\n",
        "        result: the coverage result\n",
        "    \n",
        "    \"\"\"\n",
        "    net.eval()\n",
        "    coverage_recorder = collections.OrderedDict()\n",
        "    batch_num = len(dataloader)\n",
        "    for idx, (data, label) in enumerate(dataloader):\n",
        "        with torch.no_grad():\n",
        "            logits, out_list, name_list = net.feature_list(data)\n",
        "        if idx % 100 == 0:\n",
        "            print(\"* finish {}/{} proportion\".format(idx, batch_num))\n",
        "        \n",
        "        # Iterate through all output layers in the output list\n",
        "        for layer_id in range(len(out_list)):\n",
        "            # record the name of current layer\n",
        "            cur_layer = name_list[layer_id]\n",
        "            cur_neuron_num = 1\n",
        "            \n",
        "            '''\n",
        "            Please compute the number of neurons in the DNN and store the result in cur_neuron_num.\n",
        "            Compute neurons_max and neurons_min based on **out_list**\n",
        "            '''\n",
        "\n",
        "            # ======Below is TODO====== #\n",
        "            if len(out_list[layer_id].shape) == 4:\n",
        "              cur_neuron_num = out_list[layer_id].shape[1] * out_list[layer_id].shape[2] * out_list[layer_id].shape[3]\n",
        "            else:\n",
        "              cur_neuron_num = out_list[layer_id].shape[1]   \n",
        "\n",
        "            out_list[layer_id] = torch.flatten(out_list[layer_id],start_dim=1)\n",
        "            neurons_max = torch.max(out_list[layer_id],0)\n",
        "            neurons_min = torch.min(out_list[layer_id],0)\n",
        "            # ======Above is TODO====== #\n",
        "\n",
        "            # Compare the result in cov_dict and record neurons who has been covered.\n",
        "            for neuron_id in range(cur_neuron_num):\n",
        "                if (cur_layer, neuron_id) not in coverage_recorder:\n",
        "                    # Init the coverage_recorder\n",
        "                    # [(cur_layer, neuron_id)] = [0, 0] (LowerCornerNeuron, UpperCornerNeuron)\n",
        "                    coverage_recorder[(cur_layer, neuron_id)] = [0, 0] # LowerCornerNeuron, UpperCornerNeuron\n",
        "                \n",
        "                '''\n",
        "                Compare the output in neurons_min(neurons_max) and and cov_dict\n",
        "                If the neuron is covered in LowerCorner, set LowerCornerNeuron = 1\n",
        "                If the neuron is covered in UpperCorner, set UpperCornerNeuron = 1\n",
        "                Data sturcture used here: cov_dict, neurons_min, neurons_max, coverage_recorder\n",
        "                '''\n",
        "                \n",
        "                # ======Below is TODO====== #\n",
        "                if neurons_min.values[neuron_id] < cov_dict[(cur_layer, neuron_id)][0]:\n",
        "                  coverage_recorder[(cur_layer, neuron_id)][0] = 1\n",
        "                if neurons_max.values[neuron_id] > cov_dict[(cur_layer, neuron_id)][1]:\n",
        "                  coverage_recorder[(cur_layer, neuron_id)][1] = 1  \n",
        "                # ======Above is TODO====== #\n",
        "    \n",
        "    num_of_coverage_neuron = 0\n",
        "    \n",
        "    '''\n",
        "    Compute the number of neurons that are covered\n",
        "    Data sturcture used here: coverage_recorder, result\n",
        "    '''\n",
        "    \n",
        "    # ======Below is TODO====== #\n",
        "    num_of_coverage_neuron = 0\n",
        "    N = len(coverage_recorder.values())\n",
        "    for value in coverage_recorder.values():\n",
        "      num_of_coverage_neuron += sum(value)\n",
        "    result = num_of_coverage_neuron/(2*N)\n",
        "    # ======Above is TODO====== #\n",
        "    \n",
        "    return result"
      ],
      "id": "1f17b3f7"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f1c4189",
        "outputId": "e458632e-b403-4cae-b239-ab9521091741"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* finish 0/157 proportion\n",
            "* finish 100/157 proportion\n",
            "1402\n",
            "Your cov result is 0.14933958244567533\n"
          ]
        }
      ],
      "source": [
        "cov_result = compute_NBCov(model, test_loader, train_cov_dict)\n",
        "print(\"Your cov result is {}\".format(cov_result))"
      ],
      "id": "0f1c4189"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01bf737a"
      },
      "source": [
        "### 2.2 Mutation <font color='red' size=3> (6 points)</font>"
      ],
      "id": "01bf737a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0522196"
      },
      "source": [
        "We have successfully implemented one testing metric. Now let's move to the next task: test generation.\n",
        "\n",
        "As we've mentioned before, the size of test set is usually limited, but it's necessary to have more test cases if we want to know whether the trained DNN works well in a diverse environment. \n",
        "\n",
        "In traditional software testing, there is a technique called [mutation testing](https://en.wikipedia.org/wiki/Mutation_testing). Based on a limited number of initial testing programs (seeds), we can apply mutations (modifying a program in small ways) on them to get more testing cases.\n",
        "\n",
        "In mutation testing, there are two things to keep in mind:\n",
        "\n",
        "1. How to define the mutation operations, so the test cases after mutation is valid.\n",
        "\n",
        "2. What's the expeceted behavior of the system under test for the mutated result?"
      ],
      "id": "f0522196"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5d0d9a2"
      },
      "source": [
        "#### 2.2.1 Mutation Testing: generate valid test cases <font color='red' size=3> (6 points)</font>"
      ],
      "id": "d5d0d9a2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9840574d"
      },
      "source": [
        "#### Mutation on images:\n",
        "\n",
        "Since we are testing an image classifier, we need to search for transformations that are capable of transforming an image while keeping its semantics. There are many choices available, for example:\n",
        "    \n",
        "1. image_brightness: adjust the brightness of image.\n",
        "\n",
        "2. image_noise: add some noise to the image.\n",
        "\n",
        "3. image_scale: scale the image.\n",
        "\n",
        "4. ..."
      ],
      "id": "9840574d"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "543d739d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "73d8e4c9-ed17-4299-c229-738ef5a3bfe8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Let's see some examples\n",
        "# This is image_scale\n",
        "from all_utils import image_scale\n",
        "seed = np.expand_dims(number_a, 0)\n",
        "result = image_scale(seed, 2)\n",
        "plot_image(result)"
      ],
      "id": "543d739d"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "65107b0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "37c07610-1469-42c8-ee46-6d8ec74af592"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Let's see some examples\n",
        "# This is image_noise\n",
        "from all_utils import image_noise\n",
        "seed = np.expand_dims(number_a, 0)\n",
        "result = image_noise(seed, 1)\n",
        "plot_image(result)"
      ],
      "id": "65107b0f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f78695d5"
      },
      "source": [
        "These transformations are already been implemented, which could be classified as two types: \n",
        "\n",
        "class A (pixel value transformation).\n",
        "\n",
        "class B (Affine transformation)."
      ],
      "id": "f78695d5"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "3044e352"
      },
      "outputs": [],
      "source": [
        "from all_utils import Mutators\n",
        "mutator_instance = Mutators()"
      ],
      "id": "3044e352"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "208c44a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb21a071-c381-4db1-eab9-6b3676097463"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===Pixel value transformation===\n",
            "image_pixel_change\n",
            "image_noise\n",
            "\n",
            "===Affine transformation===\n",
            "image_scale\n",
            "image_shear\n",
            "image_rotation\n",
            "image_contrast\n",
            "image_brightness\n",
            "image_blur\n"
          ]
        }
      ],
      "source": [
        "print(\"===Pixel value transformation===\")\n",
        "for i in mutator_instance.classA:\n",
        "    print(mutator_instance.transformations[i].__name__)\n",
        "print()\n",
        "print(\"===Affine transformation===\")\n",
        "for i in mutator_instance.classB:\n",
        "    print(mutator_instance.transformations[i].__name__)"
      ],
      "id": "208c44a2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8189bce2"
      },
      "source": [
        "We can easily define what is valid test cases for classical software. For example, a program that add `int a` with `int b` should not receive two imaginary numbers as input. But what is \"valid\" for DNN? Let's take a look at some definitions from DeepHunter."
      ],
      "id": "8189bce2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f90c3808"
      },
      "source": [
        "<font color='red' size=3> Definition 5</font> \n",
        "\n",
        "Given an ideal human oracle O and a test x of a specific input domain, we define metamorphic mutation $M$ on $x$, if\n",
        "$\\forall x  \\in M(x)$, we have $O(x') = O(x)$ .\n",
        "\n",
        "This is to say, the metamorphic mutation does not change the semantic of the image. At least human should identify $x$ and $x'$ as the same image"
      ],
      "id": "f90c3808"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f03d080"
      },
      "source": [
        "<font color='red' size=3> Definition 6</font> \n",
        "\n",
        "An image $s'$ is one-time mutated from $s$ if $s'$ is generated after a transformation $t$ on $s$ (denoted as $s \\stackrel{t}{\\longrightarrow}$ $s'$). An image $s'$ is sequentially mutated from s if $s'$ is generated after a sequence of one-time mutations (denoted as $s \\stackrel{t_0, t_1, \\ldots, t_n}{\\longrightarrow}$ $s'$).\n",
        "\n",
        "Allow sequential mutataion can greatly improve the diversity of th testing"
      ],
      "id": "1f03d080"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f84d30d0"
      },
      "source": [
        "<font color='green' size=3> Summary: No matter for one-time mutations or a sequence of mutations, we have to ensure that they are metamorphic mutations</font> "
      ],
      "id": "f84d30d0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5c683bd"
      },
      "source": [
        "<font color='red' size=3> Definition 7</font>\n",
        "\n",
        "The image $s'$ after transformation, and the original image $s$ are the same if:\n",
        "\n",
        "1. There are no more than one affine transformation between $s'$ and $s$.\n",
        "\n",
        "2. The pixel value changes between $s'$ and $s$ satisfied:\n",
        "\n",
        "\\begin{equation*}\n",
        "    f(s, s') = \\left\\{\\begin{array}{lr}\n",
        "        L_{\\infty}(s, s')  \\leq 255 &\\qquad \\text{if} \n",
        "        \\quad L_0(s, s') < \\alpha \\times size(s) \\\\\n",
        "        L_{\\infty}(s, s') < \\beta \\times 255 & \\text{otherwise} \\\\\n",
        "        \\end{array}\\right\\} \\\\\n",
        "\\end{equation*}\n",
        "\n",
        "(Equation 1 in [DeepHunter: A Coverage-Guided Fuzz Testing Framework for Deep Neural Networks](https://dl.acm.org/doi/pdf/10.1145/3293882.3330579?casa_token=mGUOGHi9qrcAAAAA:G28BWlY6XueMERjYHKWyW7w5jfGQrzF3fyml6ThPX0Rq8yUE4NYTz28s83fllXvaRItcpflNbBtdEYs))\n",
        "\n",
        "Where:\n",
        "\n",
        "\\begin{equation*}\n",
        "\\begin{aligned}\n",
        "    L_0(s, s') &= L_0(s_0, s_{j-1}) + L_0(s_j, s') \\\\\n",
        "    L_{\\infty}(s, s') &= L_0(s_0, s_{j-1}) + L_0(s_j, s') \\\\\n",
        "\\end{aligned}\n",
        "\\end{equation*}\n",
        "\n",
        "$s_{j-1}$ is the transformed image before one affine transformation\n",
        "\n",
        "$L_0$ and $L_{\\infty}$ is the 0-norm and infinity-norm. $L_0(a, b) = sum((a - b) != 0)$, $L_0(a, b) = sum((a - b) != 0)$, $L_{\\infty}(a, b) = max((a-b))$"
      ],
      "id": "a5c683bd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c316587f"
      },
      "source": [
        "The intuition behind Definition 7:\n",
        "\n",
        "1. The pixel value should not be changed too much, so the semantic of the transformed image does not change. This change is measured by the subtraction.\n",
        "\n",
        "2. Affine transformation will change the position of each pixel, making the norm measurement inaccurate (Take a look at `image_scale` example). As a result, we cut the sequence of transformations into two parts, measuring each independently."
      ],
      "id": "c316587f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87df8163"
      },
      "source": [
        "Based on all the definitions above, we have Algorithm 2 in [DeepHunter: A Coverage-Guided Fuzz Testing Framework for Deep Neural Networks](https://dl.acm.org/doi/pdf/10.1145/3293882.3330579?casa_token=mGUOGHi9qrcAAAAA:G28BWlY6XueMERjYHKWyW7w5jfGQrzF3fyml6ThPX0Rq8yUE4NYTz28s83fllXvaRItcpflNbBtdEYs)"
      ],
      "id": "87df8163"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "184dacef"
      },
      "source": [
        "![Title](https://raw.githubusercontent.com/momentum-openspace/momentum-openspace.github.io/new-template/files/asg2/pic/alg_2.png)"
      ],
      "id": "184dacef"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17dfa018"
      },
      "source": [
        "<font color='blue' size=4> Please answer the following question</font> <font color='red' size=4> (2 points)</font>"
      ],
      "id": "17dfa018"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d1907bb"
      },
      "source": [
        "<font color='blue' size=3> For all images returned in Algorithm 2, how many mutations are applied for each image if the success flag is True ? How does sequential mutations done in Deephunter? </font>"
      ],
      "id": "8d1907bb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c29f72fd"
      },
      "source": [
        "Hint: Take a look at algorithm 1"
      ],
      "id": "c29f72fd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb91f542"
      },
      "source": [
        "![Title](https://raw.githubusercontent.com/momentum-openspace/momentum-openspace.github.io/new-template/files/asg2/pic/alg_1.png)"
      ],
      "id": "eb91f542"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f631676"
      },
      "source": [
        "<font color='blue' size=4> Please complete the todo listed in the function shown below</font> <font color='red' size=4> (4 points)</font>"
      ],
      "id": "0f631676"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5e427df"
      },
      "source": [
        "<font color='blue' size=3> Please also attach three images after mutating in the final report</font> "
      ],
      "id": "e5e427df"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "f08f8c2d"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from all_utils import Mutators\n",
        "import cv2\n",
        "import time\n",
        "import copy\n",
        "\n",
        "# We are going to implement Line 5 ~ 18 in Algorithm 2 in the original paper.\n",
        "\n",
        "\n",
        "def mutate_one(ref_img, \n",
        "               img, \n",
        "               has_Affine, \n",
        "               l0_ref, \n",
        "               linf_ref,\n",
        "               alpha=0.02,\n",
        "               beta=0.20, \n",
        "               try_num=50):\n",
        "    \"\"\"\n",
        "    Mutate the image for once.\n",
        "    Line 5 ~ 18 in Algorithm 2 in the original paper.\n",
        "    Args\n",
        "    ---\n",
        "        ref_img: reference image. s0' in the original paper. In the format (C, H, W)\n",
        "            Notice: s0 is the original reference image and we actually does not need it here.\n",
        "        img: the seed. s in the original paper. In the format (C, H, W)\n",
        "        has_Affine: Flag for indicating whether we have applied affine transformation on the image.\n",
        "        l0_ref: The value for L_0(s_0, s_{j-1}). It is used to compute L_0(s_0, s_n) in the equation 2.\n",
        "        linf_ref: The value for L_infinity(s_0, s_{j-1}). It is used to \n",
        "            compute L_infinity(s_0, s_n) in the equation 2.\n",
        "        alpha: alpha in equation 1.\n",
        "        betra: beta in equation 1.\n",
        "        try_num: TRY_NUM in Algorithm 2.\n",
        "        \n",
        "    Returns:\n",
        "        ref_img: reference image. s0' in the original paper.\n",
        "        img: the image after transformation\n",
        "        has_Affine: Updated flag\n",
        "        changed: whether there is actually change for the image.\n",
        "        l0_ref: updated l0_ref.\n",
        "        linf_ref: updated line_ref.\n",
        "    \"\"\"\n",
        "    # ref_img is the reference image, img is the seed\n",
        "\n",
        "    # cl means the current state of transformation\n",
        "    # 0 means it can select both of Affine and Pixel transformations\n",
        "    # 1 means it only select pixel transformation because an Affine transformation has been used before\n",
        "\n",
        "    # l0_ref, linf_ref: if the current seed is mutated from affine transformation, we will record the l0, l_inf\n",
        "    # between initial image and the reference image. i.e., L0(s_0,s_{j-1}) L_inf(s_0,s_{j-1}) in Equation 2 of the paper\n",
        "\n",
        "    # tyr_num is the maximum number of trials in Algorithm 2\n",
        "    \n",
        "    if isinstance(ref_img, torch.Tensor):\n",
        "        ref_img = ref_img.numpy()\n",
        "        img = img.numpy()\n",
        "        is_torch = True\n",
        "    else:\n",
        "        is_torch = False\n",
        "        \n",
        "    c, h, w = img.shape\n",
        "    mutator_instance = Mutators()\n",
        "    # a, b is the alpha and beta in Equation 1 in the paper\n",
        "\n",
        "    # l0_threshold: alpha * size(s), l_infinity_threshold: beta * 255 in Equation 1\n",
        "    l0_threshold = int(alpha * c * h * w)\n",
        "    l_infinity_threshold = int(beta * 255)\n",
        "    ori_shape = ref_img.shape\n",
        "    \n",
        "    for ii in range(try_num):\n",
        "        random.seed(time.time())\n",
        "        if has_Affine == 0:  # 0: can choose class A and B Line-6 of the algorithm\n",
        "            tid = random.sample(mutator_instance.classA + mutator_instance.classB, 1)[0]\n",
        "            # Randomly select one transformation   Line-7 in Algorithm2\n",
        "            transformation = mutator_instance.transformations[tid]\n",
        "            params = mutator_instance.params[tid]\n",
        "            # Randomly select one parameter Line 10 in Algo2\n",
        "            param = random.sample(params, 1)[0]\n",
        "\n",
        "            # Perform the transformation  Line 11 in Algo2\n",
        "            img_new = transformation(copy.deepcopy(img), param)\n",
        "            img_new = img_new.reshape(ori_shape)\n",
        "            \n",
        "            \n",
        "            '''\n",
        "            Notice: the Equation 1 and Line 12 in Algo2 is satisfied \n",
        "            if the transformation is affine transformation\n",
        "            So the line 12 of the algorithm is moved down.\n",
        "            '''\n",
        "            \n",
        "            if tid in mutator_instance.classA: # Line\n",
        "                \n",
        "                '''\n",
        "                check whether it is a valid mutation. i.e., Equation 1 and Line 12 in Algo2\n",
        "                You need to compute l0_ref, linf_ref, and compare them with their own\n",
        "                threshold.\n",
        "                Data structure used here:  ref_img, img_new, l0_ref, l0_threshold\n",
        "                linf_ref, l_infinity_threshold\n",
        "                '''\n",
        "                \n",
        "                # ======Below is TODO====== #\n",
        "               \n",
        "                l0 = np.linalg.norm((img_new-ref_img).flatten(), ord = 0)\n",
        "                l0_ref = l0 + l0_ref\n",
        "                linf = np.linalg.norm((img_new-ref_img).flatten(), ord = np.inf)\n",
        "                linf_ref = linf + linf_ref\n",
        "                if (linf_ref <= 255 and l0_ref < l0_threshold) or (linf_ref < l_infinity_threshold and l0_ref  >= l0_threshold): \n",
        "                # line 12 of the algorithm 2. We move the comparision down here.\n",
        "                  return ref_img, img_new, has_Affine, True, l0_ref, linf_ref # Please fill the return field.\n",
        "                # ======Above is TODO====== #\n",
        "                \n",
        "                else:  # B, C\n",
        "                \n",
        "                  '''\n",
        "                  If the current transformation is an Affine trans, we will update the reference image and\n",
        "                  the transformation state of the seed.\n",
        "                  '''\n",
        "                \n",
        "                  ref_img = transformation(copy.deepcopy(ref_img), param)\n",
        "                  ref_img = ref_img.reshape(ori_shape)\n",
        "                    \n",
        "                  # ======Below is TODO====== #\n",
        "                  return ref_img, img_new, 1, True, l0_ref, linf_ref # Please fill the return field.\n",
        "                  # ======Above is TODO====== #\n",
        "      \n",
        "        elif has_Affine == 1: # 0: can only choose class A. Line 6 of the original algorithm fails.\n",
        "          tid = random.sample(mutator_instance.classA, 1)[0]\n",
        "          transformation = mutator_instance.transformations[tid]\n",
        "          params = mutator_instance.params[tid]\n",
        "          param = random.sample(params, 1)[0]\n",
        "          img_new = transformation(copy.deepcopy(img), param)\n",
        "            \n",
        "            \n",
        "          '''\n",
        "          Compute the value in Equation 2 in the paper.\n",
        "          So the comparision can be done below.\n",
        "          Data structure used here: ref_img, img_new, l0_ref, linf_ref\n",
        "          '''\n",
        "            \n",
        "          # ======Below is TODO====== #\n",
        "          l0 = np.linalg.norm((img_new-ref_img).flatten(), ord = 0)\n",
        "          l0_ref = l0 + l0_ref\n",
        "          linf = np.linalg.norm((img_new-ref_img).flatten(), ord = np.inf)\n",
        "          linf_ref = linf + linf_ref\n",
        "          if (linf_ref <= 255 and l0_ref < l0_threshold) or (linf_ref < l_infinity_threshold and l0_ref >= l0_threshold):\n",
        "             # The if statement is satisified, we can return the result.\n",
        "            return ref_img, img_new, has_Affine, True, l0_ref, linf_ref # Please fill the return field.\n",
        "            \n",
        "          # ======Above is TODO====== #\n",
        "  # Otherwise the mutation is failed. Line 20 in Algo 2\n",
        "    return ref_img, img, has_Affine, False, l0_ref, linf_ref"
      ],
      "id": "f08f8c2d"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "0c501087"
      },
      "outputs": [],
      "source": [
        "seed = np.expand_dims(number_a, 0)"
      ],
      "id": "0c501087"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "830608c6"
      },
      "outputs": [],
      "source": [
        "ref_img_list = list()\n",
        "new_img_list = list()\n",
        "affline_flag_list = list()\n",
        "l0_ref_list = list()\n",
        "linf_ref_list = list()"
      ],
      "id": "830608c6"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "30adf1ff"
      },
      "outputs": [],
      "source": [
        "result = mutate_one(\n",
        "        seed,\n",
        "        seed,\n",
        "        False,\n",
        "        0,\n",
        "        0\n",
        ")\n",
        "result = all_utils.turn_numpy_into_torch_array(result)\n",
        "# Below are the data structures that store the necessary info\n",
        "# for future mutation.\n",
        "ref_img_list.append(result[0])\n",
        "new_img_list.append(result[1]) # T in the original paper\n",
        "affline_flag_list.append(result[2])\n",
        "l0_ref_list.append(result[3])\n",
        "linf_ref_list.append(result[4])"
      ],
      "id": "30adf1ff"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "3a93bfa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "ce0674ef-b2d2-422b-c2b7-5b6975441e6f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plot_image(result[1])"
      ],
      "id": "3a93bfa6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ada28a2"
      },
      "source": [
        "## 3. Experiments <font color='red' size=5> (3 points)</font>"
      ],
      "id": "2ada28a2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1793ae4"
      },
      "source": [
        "In this section, you are asked to use `Algorithm 2` to do the mutation testing and use function `compute_NBCov` to evaluate the results.\n",
        "\n",
        "There are multiple ways to do the mutation testing:\n",
        "\n",
        "1. Just mutate every image for once in the test set. And combine the test set and mutated images as new dataset for testing.\n",
        "\n",
        "2. Implement Algorithm 1 in the original paper to guide the testing.\n",
        "\n",
        "3. Random select part of the images for testing.\n",
        "\n",
        "4. ...\n",
        "\n",
        "This is an open-ended question, feel free to use any ideas that come to your mind"
      ],
      "id": "f1793ae4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b746137a"
      },
      "source": [
        "Here is an example to build self-defined dataset."
      ],
      "id": "b746137a"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "b030645e"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset"
      ],
      "id": "b030645e"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "264a4d14"
      },
      "outputs": [],
      "source": [
        "test_image_set = list()\n",
        "labels = list()\n",
        "\n",
        "for imgs, label in test_loader:\n",
        "    for i in range(len(imgs)):\n",
        "        test_image_set.append(imgs[i].unsqueeze(0))\n",
        "        labels.append(int(label[i]))\n",
        "        \n",
        "        # ======Below is TODO====== #\n",
        "        # Please remember to use `result = all_utils.turn_numpy_into_torch_array(result)`\n",
        "        result = mutate_one(\n",
        "        imgs[i],\n",
        "        imgs[i],\n",
        "        False,\n",
        "        0,\n",
        "        0)   \n",
        "        result = all_utils.turn_numpy_into_torch_array(result)\n",
        "        test_image_set.append(result[1].unsqueeze(0))\n",
        "        labels.append(int(label[i]))\n",
        "        # ======Above is TODO====== #"
      ],
      "id": "264a4d14"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "cfa5a26c"
      },
      "outputs": [],
      "source": [
        "new_test_dataset = TensorDataset(torch.cat(test_image_set), torch.tensor(labels))\n",
        "new_test_dataloader = torch.utils.data.DataLoader(new_test_dataset, batch_size=64)"
      ],
      "id": "cfa5a26c"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "9e58e7ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53c2e87c-de5b-4f68-adf4-6feba1ef0e41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* finish 0/313 proportion\n",
            "* finish 100/313 proportion\n",
            "* finish 200/313 proportion\n",
            "* finish 300/313 proportion\n",
            "8812\n"
          ]
        }
      ],
      "source": [
        "# train_cov_dict is obtained at section 2.1.2\n",
        "cov_result = compute_NBCov(model, new_test_dataloader, train_cov_dict)"
      ],
      "id": "9e58e7ca"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ePN1txNn9jjN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "356f879b-62e8-4646-c792-8c5b40d63b9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9386450788240307\n"
          ]
        }
      ],
      "source": [
        "print(cov_result)"
      ],
      "id": "ePN1txNn9jjN"
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "HW2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}